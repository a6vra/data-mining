

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Preprocessing: from covariance matrix to image whitening &#8212; Data Mining</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../../../../_static/styles/bootstrap.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />

  
  <link href="../../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=ac02cc09edc035673794" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794" />
<link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794" />
  <script src="../../../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=ac02cc09edc035673794"></script>

    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script src="../../../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'aiml-common/lectures/pca/whitening/preprocessing_workshop';</script>
    <link rel="canonical" href="https://pantelis.github.io/data-mining/aiml-common/lectures/pca/whitening/preprocessing_workshop.html" />
    <link rel="shortcut icon" href="../../../../_static/logo.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Introduction to Recommender Systems" href="../../recommenders/recommenders-intro/_index.html" />
    <link rel="prev" title="Data Preprocessing" href="index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../../../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../../../../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../../../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../../intro.html">
                    Data Mining
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Syllabus</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../syllabus/index.html">Syllabus</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction to Data Mining</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../data-premise/index.html">The new premise</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ai-intro/data-science-360/_index.html">Data Science 360</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../pipelines/_index.html">Pipelines</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../pipelines/uber-ml-arch-case-study/index.html">A Case Study of an ML Architecture - Uber</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../pipelines/01_the_machine_learning_landscape.html">The Machine Learning landscape</a></li>



</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">The Learning Problem</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../learning-problem/_index.html">The Learning Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pipelines/02_end_to_end_machine_learning_project.html">End-to-end Machine Learning project</a></li>






<li class="toctree-l1"><a class="reference internal" href="../../model-selection/bias_variance.html">Empirical Risk Minimization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Predictors for Structured Data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../entropy/_index.html">Entropy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../trees/decision-trees/index.html">Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../trees/decision-trees/decision_tree.html">Decision tree from scratch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../trees/decision-trees/decision_tree_visualisations.html">Visualizing tree-based classifiers</a></li>

<li class="toctree-l1"><a class="reference internal" href="../../trees/regression-trees/regression_tree_visualisations.html">Visualizing tree-based regressors</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../ensemble/index.html">Ensemble Methods</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../ensemble/random-forests/index.html">Random Forests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ensemble/adaboost/index.html">Adaptive Boosting (AdaBoost)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ensemble/adaboost/adaboost_example.html">Adaboost from scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ensemble/gradient-boosting/index.html">Gradient Boosting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ensemble/boosting-workshop/index.html">Boosting Workshop</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Learning without Labels or Without Parameters</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../unsupervised/k-means/_index.html">K-means Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../density-estimation/knn/_index.html">k-Nearest Neighbors (kNN) Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../density-estimation/knn-workshop/_index.html">kNN Workshop</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Dimensionality Reduction</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../introduction/index.html">Principal Component Analysis (PCA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/principal_component_analysis.html">PCA Workshop</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html">Data Preprocessing</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Preprocessing: from covariance matrix to image whitening</a></li>




<li class="toctree-l1"><a class="reference internal" href="../../recommenders/recommenders-intro/_index.html">Introduction to Recommender Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../recommenders/netflix/_index.html">The Netflix Prize and Singular Value Decomposition</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Regression and Classification</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../regression/linear-regression/linear_regression.html">Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../optimization/sgd/_index.html">Stochastic Gradient Descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../classification/classification-intro/_index.html">Introduction to Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../classification/logistic-regression/_index.html">Logistic Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../classification/perceptron/_index.html">The Neuron (Perceptron)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dnn/dnn-intro/_index.html">Deep Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dnn/fashion-mnist-case-study.html">Fashion MNIST Case Study</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../optimization/regularization/_index.html">Regularization in Deep Neural Networks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Convolutional Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../cnn/cnn-intro/_index.html">Introduction to Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cnn/cnn-layers/_index.html">CNN Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cnn/cnn-example-architectures/_index.html">CNN Example Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cnn/cnn-example-architectures/using_convnets_with_small_datasets.html">Using convnets with small datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../scene-understanding/feature-extraction-resnet/index.html">Feature Extraction via Residual Networks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Representation Learning and Autoencoders</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../vae/index.html">Variational Auto Encoders (VAE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../vae/generative-modeling/index.html">Generative Modeling and Approximate Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../vae/vae-architecture/index.html">VAE Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../vae/elbo-optimization/index.html">ELBO Optimization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Sequences and RNNs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../rnn/introduction/_index.html">Introduction to Recurrent Neural Networks (RNN)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rnn/simple-rnn/_index.html">Simple RNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rnn/lstm/_index.html">The Long Short-Term Memory (LSTM) Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rnn/time_series_using_simple_rnn_lstm.html">Time Series Prediction using RNNs</a></li>





</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Natural Language Processing</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../nlp/nlp-introduction/nlp-pipelines/_index.html">Introduction to NLP Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nlp/nlp-introduction/tokenization/index.html">Tokenization</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp/nlp-introduction/word2vec/_index.html">Embeddings</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../nlp/nlp-introduction/word2vec/word2vec_from_scratch.html">Word2Vec from scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp/nlp-introduction/word2vec/word2vec_tensorflow_tutorial.html">Word2Vec Tensorflow Tutorial</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp/language-models/_index.html">Language Models</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../nlp/language-models/cnn-language-model/index.html">CNN Language Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp/language-models/simple-rnn-language-model/index.html">Simple RNN Language Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp/language-models/lstm-language-model/index.html">LSTM Language Model from scratch</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Math Background</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../ml-math/index.html">Math for ML Textbook</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ml-math/probability/index.html">Probability Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ml-math/linear-algebra/index.html">Linear Algebra for Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ml-math/calculus/index.html">Calculus</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../resources/environment/index.html">Your Programming Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../resources/environment/assignment-submission.html">Submitting Your Assignment / Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python/index.html">Learn Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../resources/environment/notebook-status.html">Notebook execution status</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Assignments</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../assignments/probability/probability-assignment-7/index.html">Probability Assignment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../assignments/ensemble/ais/index.html">Classifying Ships from Automatic Indentification System (AIS) Data</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/pantelis/data-mining/master?urlpath=tree/data_mining/aiml-common/lectures/pca/whitening/preprocessing_workshop.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../../../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/pantelis/data-mining/blob/master/data_mining/aiml-common/lectures/pca/whitening/preprocessing_workshop.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/pantelis/data-mining" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pantelis/data-mining/edit/master/data_mining/aiml-common/lectures/pca/whitening/preprocessing_workshop.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pantelis/data-mining/issues/new?title=Issue%20on%20page%20%2Faiml-common/lectures/pca/whitening/preprocessing_workshop.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../../_sources/aiml-common/lectures/pca/whitening/preprocessing_workshop.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Preprocessing: from covariance matrix to image whitening</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Preprocessing: from covariance matrix to image whitening</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#background">1. Background</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-variance-and-covariance">A. Variance and covariance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1">Example 1.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-covariance-matrix-with-the-dot-product">Finding the covariance matrix with the dot product</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#b-visualize-data-and-covariance-matrices">B. Visualize data and covariance matrices</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#c-simulating-data">C. Simulating data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#uncorrelated-data">Uncorrelated data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#correlated-data">Correlated data</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocessing">2. Preprocessing</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-mean-normalization">A. Mean normalization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#b-standardization">B. Standardization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#c-whitening">C. Whitening</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-centering">1. Zero-centering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decorrelate">2. Decorrelate</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rescale-the-data">3. Rescale the data</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#image-whitening">3. Image whitening</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-subtraction-per-pixel-or-per-image">Mean subtraction: per-pixel or per-image?</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;axes.facecolor&#39;</span><span class="p">]</span><span class="o">=</span><span class="s1">&#39;w&#39;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;axes.edgecolor&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;#D6D6D6&#39;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;axes.linewidth&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>
</pre></div>
</div>
</div>
</div>
<div class="math notranslate nohighlight">
\[
\newcommand\bs[1]{\boldsymbol{#1}}
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%html</span>
<span class="p">&lt;</span><span class="nt">style</span><span class="p">&gt;</span>
<span class="p">.</span><span class="nc">pquote</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">text-align</span><span class="p">:</span><span class="w"> </span><span class="kc">left</span><span class="p">;</span>
<span class="w">  </span><span class="k">margin</span><span class="p">:</span><span class="w"> </span><span class="mi">40</span><span class="kt">px</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="mi">40</span><span class="kt">px</span><span class="w"> </span><span class="kc">auto</span><span class="p">;</span>
<span class="w">  </span><span class="k">width</span><span class="p">:</span><span class="w"> </span><span class="mi">70</span><span class="kt">%</span><span class="p">;</span>
<span class="w">  </span><span class="k">font-size</span><span class="p">:</span><span class="w"> </span><span class="mf">1.5</span><span class="kt">em</span><span class="p">;</span>
<span class="w">  </span><span class="k">font-style</span><span class="p">:</span><span class="w"> </span><span class="kc">italic</span><span class="p">;</span>
<span class="w">  </span><span class="k">display</span><span class="p">:</span><span class="w"> </span><span class="kc">block</span><span class="p">;</span>
<span class="w">  </span><span class="k">line-height</span><span class="p">:</span><span class="w"> </span><span class="mf">1.3</span><span class="kt">em</span><span class="p">;</span>
<span class="w">  </span><span class="k">color</span><span class="p">:</span><span class="w"> </span><span class="mh">#5a75a7</span><span class="p">;</span>
<span class="w">  </span><span class="k">font-weight</span><span class="p">:</span><span class="w"> </span><span class="mi">600</span><span class="p">;</span>
<span class="w">  </span><span class="k">border-left</span><span class="p">:</span><span class="w"> </span><span class="mi">5</span><span class="kt">px</span><span class="w"> </span><span class="kc">solid</span><span class="w"> </span><span class="nb">rgba</span><span class="p">(</span><span class="mi">90</span><span class="p">,</span><span class="w"> </span><span class="mi">117</span><span class="p">,</span><span class="w"> </span><span class="mi">167</span><span class="p">,</span><span class="w"> </span><span class="mf">.1</span><span class="p">);</span>
<span class="w">  </span><span class="k">padding-left</span><span class="p">:</span><span class="w"> </span><span class="mi">6</span><span class="kt">px</span><span class="p">;</span>
<span class="p">}</span>
<span class="p">.</span><span class="nc">notes</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">font-style</span><span class="p">:</span><span class="w"> </span><span class="kc">italic</span><span class="p">;</span>
<span class="w">  </span><span class="k">display</span><span class="p">:</span><span class="w"> </span><span class="kc">block</span><span class="p">;</span>
<span class="w">  </span><span class="k">margin</span><span class="p">:</span><span class="w"> </span><span class="mi">40</span><span class="kt">px</span><span class="w"> </span><span class="mi">10</span><span class="kt">%</span><span class="p">;</span>
<span class="p">}</span>
<span class="nt">img</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nt">em</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">text-align</span><span class="p">:</span><span class="w"> </span><span class="kc">center</span><span class="p">;</span>
<span class="w">  </span><span class="k">display</span><span class="p">:</span><span class="w"> </span><span class="kc">block</span><span class="p">;</span>
<span class="w">  </span><span class="k">color</span><span class="p">:</span><span class="w"> </span><span class="kc">gray</span><span class="p">;</span>
<span class="w">  </span><span class="k">font-size</span><span class="p">:</span><span class="w"> </span><span class="mf">0.9</span><span class="kt">em</span><span class="p">;</span>
<span class="w">  </span><span class="k">font-weight</span><span class="p">:</span><span class="w"> </span><span class="mi">600</span><span class="p">;</span>
<span class="p">}</span>
<span class="p">&lt;/</span><span class="nt">style</span><span class="p">&gt;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>
.pquote {
  text-align: left;
  margin: 40px 0 40px auto;
  width: 70%;
  font-size: 1.5em;
  font-style: italic;
  display: block;
  line-height: 1.3em;
  color: #5a75a7;
  font-weight: 600;
  border-left: 5px solid rgba(90, 117, 167, .1);
  padding-left: 6px;
}
.notes {
  font-style: italic;
  display: block;
  margin: 40px 10%;
}
img + em {
  text-align: center;
  display: block;
  color: gray;
  font-size: 0.9em;
  font-weight: 600;
}
</style>
</div></div>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="preprocessing-from-covariance-matrix-to-image-whitening">
<h1>Preprocessing: from covariance matrix to image whitening<a class="headerlink" href="#preprocessing-from-covariance-matrix-to-image-whitening" title="Permalink to this heading">#</a></h1>
<p>A version of this notebook has been published <a class="reference external" href="https://hadrienj.github.io/posts/Preprocessing-for-deep-learning/">here</a>.</p>
<p>The goal of this post/notebook is to go from the basics of data preprocessing to modern techniques used in machine learning. My point is that we can use code (Python/Numpy etc.) to better understand abstract mathematical notions! Thinking by coding! 💥</p>
<p>We will start with basic but very useful concepts in data science and machine learning learning like variance and covariance matrix and we will go further to some preprocessing techniques used to feed images into neural networks. We will try to get more concrete insights using code to actually see what each equation is doing!</p>
<p>We call preprocessing all transformations on the raw data before it is fed to the machine learning algorithm. For instance, training a convolutional neural network on raw images will probably lead to bad classification performances (<a class="reference external" href="https://ieeexplore.ieee.org/document/7808140/">Pal &amp; Sumachine, 2016</a>). The preprocessing is also important to speed up training (for instance, centering and scaling techniques, see <a class="reference external" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">Lecun et al., 2012; see 4.3</a>).</p>
<p>Here is the syllabus of this tutorial:</p>
<ol class="arabic simple">
<li><p><strong>Background</strong>: In the first part, we will get some reminders about variance and covariance and see how to generate and plot fake data to get a better understanding of these concepts.</p></li>
<li><p><strong>Preprocessing</strong>: In the second part, we will see the basics of some preprocessing techniques that can be applied to any kind of data: mean normalization, standardisation and whitening.</p></li>
<li><p><strong>Whitening images</strong>: In the third part, we will use the tools and concepts gained in 1. and 2. to do a special kind of whitening called Zero Component Analysis (ZCA). It can be used to preprocess images for machine learning. This part will be very practical and fun ☃️!</p></li>
</ol>
<p>Feel free to fork the notebook. For instance, check the shapes of the matrices each time you have a doubt :)</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="background">
<h1>1. Background<a class="headerlink" href="#background" title="Permalink to this heading">#</a></h1>
<section id="a-variance-and-covariance">
<h2>A. Variance and covariance<a class="headerlink" href="#a-variance-and-covariance" title="Permalink to this heading">#</a></h2>
<p>The variance of a variable describes how much the values are spread. The covariance is a measure that tells the amount of dependency between two variables. A positive covariance means that values of the first variable are large when values of the second variables are also large. A negative covariance means the opposite: large values from one variable are associated with small values of the other. The covariance value depends on the scale of the variable so it is hard to analyse it. It is possible to use the correlation coefficient that is easier to interpret. It is just the covariance normalized.</p>
<p><img alt="" src="../../../../_images/negative-and-positive-covariance.png" /></p>
<p><em>A positive covariance means that large values of one variable are associated with big values from the other (left). A negative covariance means that large values of one variable are associated with small values of the other one (right).</em></p>
<p>The covariance matrix is a matrix that summarizes the variances and covariances of a set of vectors and it can tell a lot of things about your variables. The diagonal corresponds to the variance of each vector:</p>
<p><img alt="" src="../../../../_images/covariance1.png" /></p>
<p><em>A matrix <span class="math notranslate nohighlight">\(\bs{A}\)</span> and its matrix of covariance. The diagonal corresponds to the variance of each column vector.</em></p>
<p>Let’s just check with the formula of the variance:</p>
<div class="math notranslate nohighlight">
\[
V(\bs{X}) = \frac{1}{n}\sum_{i=1}^{n}(x_i-\bar{x})^2
\]</div>
<p>with <span class="math notranslate nohighlight">\(n\)</span> the length of the vector, and <span class="math notranslate nohighlight">\(\bar{x}\)</span> the mean of the vector. For instance, the variance of the first column vector of <span class="math notranslate nohighlight">\(\bs{A}\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
V(\bs{A}_{:,1}) = \frac{(1-3)^2+(5-3)^2+(3-3)^2}{3} = 2.67
\]</div>
<p>This is the first cell of our covariance matrix. The second element on the diagonal corresponds of the variance of the second column vector from <span class="math notranslate nohighlight">\(\bs{A}\)</span> and so on.</p>
<p><em>Note</em>: the vectors extracted from the matrix <span class="math notranslate nohighlight">\(\bs{A}\)</span> correspond to the columns of <span class="math notranslate nohighlight">\(\bs{A}\)</span>.</p>
<p>The other cells correspond to the covariance between two column vectors from <span class="math notranslate nohighlight">\(\bs{A}\)</span>. For instance, the covariance between the first and the third column is located in the covariance matrix as the column 1 and the row 3 (or the column 3 and the row 1).</p>
<p><img alt="" src="../../../../_images/covariance2.png" /></p>
<p><em>The position in the covariance matrix. Column corresponds to the first variable and row to the second (or the opposite). The covariance between the first and the third column vector of <span class="math notranslate nohighlight">\(\bs{A}\)</span> is the element in column 1 and row 3 (or the opposite = same value).</em></p>
<p>Let’s check that the covariance between the first and the third column vector of <span class="math notranslate nohighlight">\(\bs{A}\)</span> is equal to <span class="math notranslate nohighlight">\(-2.67\)</span>. The formula of the covariance between two variables <span class="math notranslate nohighlight">\(\bs{X}\)</span> and <span class="math notranslate nohighlight">\(\bs{Y}\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
cov(\bs{X},\bs{Y}) = \frac{1}{n} \sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})
\]</div>
<p>The variables <span class="math notranslate nohighlight">\(\bs{X}\)</span> and <span class="math notranslate nohighlight">\(\bs{Y}\)</span> are the first and the third column vectors in the last example. Let’s split this formula to be sure that it is crystal clear:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\((x_1-\bar{x})\)</span>. The sum symbol means that we will iterate on the elements of the vectors. We will start with the first element (<span class="math notranslate nohighlight">\(i=1\)</span>) and calculate the first element of <span class="math notranslate nohighlight">\(\bs{X}\)</span> minus the mean of the vector <span class="math notranslate nohighlight">\(\bs{X}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\((x_1-\bar{x})(y_1-\bar{y})\)</span>. Multiply the result with the first element of <span class="math notranslate nohighlight">\(\bs{Y}\)</span> minus the mean of the vector <span class="math notranslate nohighlight">\(\bs{Y}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})\)</span>. Reiterate the process for each element of the vectors and calculate the sum of all results.</p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{1}{n} \sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})\)</span>. Divide by the number of elements in the vector.</p></li>
</ol>
<section id="example-1">
<h3>Example 1.<a class="headerlink" href="#example-1" title="Permalink to this heading">#</a></h3>
<p>Let’s start with the matrix <span class="math notranslate nohighlight">\(\bs{A}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{A}=
\begin{bmatrix}
    1 &amp; 3 &amp; 5\\\\
    5 &amp; 4 &amp; 1\\\\
    3 &amp; 8 &amp; 6
\end{bmatrix}
\end{split}\]</div>
<p>We will calculate the covariance between the first and the third column vectors:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{X} = \begin{bmatrix}
    1\\\\
    5\\\\
    3
\end{bmatrix}
\end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\begin{split}\boldsymbol{Y} = \begin{bmatrix}
    5\\\\
    1\\\\
    6
\end{bmatrix}
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\boldsymbol{\bar{x}}=3\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\bar{y}}=4\)</span> and <span class="math notranslate nohighlight">\(n=3\)</span> so we have:</p>
<div class="math notranslate nohighlight">
\[
cov(X,Y) = \frac{(1-3)(5-4)+(5-3)(1-4)+(3-3)(6-4)}{3}=\frac{-8}{3}=-2.67
\]</div>
<p>Ok, great! That the value of the covariance matrix.</p>
<p>Now the easy way! With Numpy, the covariance matrix can be calculated with the function <code class="docutils literal notranslate"><span class="pre">np.cov</span></code>. It is worth noting that if you want Numpy to use the columns as vectors, the parameter <code class="docutils literal notranslate"><span class="pre">rowvar=False</span></code> has to be used. Also, <code class="docutils literal notranslate"><span class="pre">bias=True</span></code> allows to divide by <span class="math notranslate nohighlight">\(n\)</span> and not by <span class="math notranslate nohighlight">\(n-1\)</span>.</p>
<p>Let’s create the array first:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="n">A</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[1, 3, 5],
       [5, 4, 1],
       [3, 8, 6]])
</pre></div>
</div>
</div>
</div>
<p>Now we will calculate the covariance with the Numpy function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 2.66666667,  0.66666667, -2.66666667],
       [ 0.66666667,  4.66666667,  2.33333333],
       [-2.66666667,  2.33333333,  4.66666667]])
</pre></div>
</div>
</div>
</div>
<p>Looks good!</p>
</section>
<section id="finding-the-covariance-matrix-with-the-dot-product">
<h3>Finding the covariance matrix with the dot product<a class="headerlink" href="#finding-the-covariance-matrix-with-the-dot-product" title="Permalink to this heading">#</a></h3>
<p>There is another way to compute the covariance matrix of <span class="math notranslate nohighlight">\(\bs{A}\)</span>. You can center <span class="math notranslate nohighlight">\(
\bs{A}\)</span> around 0 (subtract the mean of the vector to each element of the vector to have a vector of mean equal to 0, <em>cf</em>. below), multiply it with its own transpose and divide by the number of observations. Let’s start with an implementation and then we’ll try to understand the link with the previous equation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calculateCovariance</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">meanX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">lenX</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">meanX</span>
    <span class="n">covariance</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">/</span><span class="n">lenX</span>
    <span class="k">return</span> <span class="n">covariance</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s test it on our matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">calculateCovariance</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 2.66666667,  0.66666667, -2.66666667],
       [ 0.66666667,  4.66666667,  2.33333333],
       [-2.66666667,  2.33333333,  4.66666667]])
</pre></div>
</div>
</div>
</div>
<p>We end up with the same result as before!</p>
<p>The explanation is simple. The dot product between two vectors can be expressed:</p>
<div class="math notranslate nohighlight">
\[
\bs{X^\text{T}Y}= \sum_{i=1}^{n}(x_i)(y_i)
\]</div>
<p>That’s right, it is the sum of the products of each element of the vectors:</p>
<p><img alt="" src="../../../../_images/dot-product.png" />
<em>The dot product corresponds to the sum of the products of each element of the vectors.</em></p>
<p>If <span class="math notranslate nohighlight">\(n\)</span> is the number of elements in our vectors and that we divide by <span class="math notranslate nohighlight">\(n\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n}\bs{X^\text{T}Y}= \frac{1}{n}\sum_{i=1}^{n}(x_i)(y_i)
\]</div>
<p>You can note that this is not too far from the formula of the covariance we have seen above:</p>
<div class="math notranslate nohighlight">
\[
cov(\bs{X},\bs{Y}) = \frac{1}{n} \sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})
\]</div>
<p>The only difference is that in the covariance formula we subtract the mean of a vector to each of its elements. This is why we need to center the data before doing the dot product.</p>
<p>Now if we have a matrix <span class="math notranslate nohighlight">\(\bs{A}\)</span>, the dot product between <span class="math notranslate nohighlight">\(\bs{A}\)</span> and its transpose will give you a new matrix:</p>
<p><img alt="" src="../../../../_images/covariance-dot-product.png" /></p>
<p><em>If you start with a zero-centered matrix, the dot product between this matrix and its transpose will give you the variance of each vector and covariance between them, that is to say the covariance matrix.</em></p>
<p>This is the covariance matrix! 🌵</p>
</section>
</section>
<section id="b-visualize-data-and-covariance-matrices">
<h2>B. Visualize data and covariance matrices<a class="headerlink" href="#b-visualize-data-and-covariance-matrices" title="Permalink to this heading">#</a></h2>
<p>In order to get more insights about the covariance matrix and how it can be useful, we will create a function used to visualize it along with 2D data. You will be able to see the link between the covariance matrix and the data.</p>
<p>This function will calculate the covariance matrix as we have seen above. It will create two subplots: one for the covariance matrix and one for the data. The <code class="docutils literal notranslate"><span class="pre">heatmap</span></code> function from Seaborn is used to create gradients of color: small values will be colored in light green and large values in dark blue. The data is represented as a scatterplot. We choose one of our palette colors, but you may prefer other colors 🌈.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plotDataAndCov</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">ACov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Covariance matrix:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ACov</span><span class="p">)</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="n">ax0</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Choosing the colors</span>
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s2">&quot;GnBu&quot;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">ACov</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    
    <span class="c1"># data can include the colors</span>
    <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="mi">3</span><span class="p">:</span>
        <span class="n">c</span><span class="o">=</span><span class="n">data</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">c</span><span class="o">=</span><span class="s2">&quot;#0A98BE&quot;</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
    
    <span class="c1"># Remove the top and right axes from the data plot</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;right&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;top&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="c-simulating-data">
<h2>C. Simulating data<a class="headerlink" href="#c-simulating-data" title="Permalink to this heading">#</a></h2>
<section id="uncorrelated-data">
<h3>Uncorrelated data<a class="headerlink" href="#uncorrelated-data" title="Permalink to this heading">#</a></h3>
<p>Now that we have the plot function, we will generate some random data to visualize what the covariance matrice can tell us. We will start with some data drawn from a normal distribution with the Numpy function <code class="docutils literal notranslate"><span class="pre">np.random.normal()</span></code>.</p>
<p><img alt="" src="../../../../_images/np.random.normal.png" />
<em>Drawing sample from a normal distribution with Numpy.</em></p>
<p>This function needs the mean, the standard deviation and the number of observations of the distribution as input. We will create two random variables of 300 observations with a standard deviation of 1. The first will have a mean of 1 and the second a mean of 2. If we draw two times 300 observations from a normal distribution, both vectors will be uncorrelated.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">a1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
<span class="n">a2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">a1</span><span class="p">,</span> <span class="n">a2</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
<span class="n">A</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(300, 2)
</pre></div>
</div>
</div>
</div>
<p><em>Note 1</em>:  We transpose the data with <code class="docutils literal notranslate"><span class="pre">.T</span></code> because the original shape is (2, 300) and we want the number of observations as rows (so with shape (300, 2)).</p>
<p><em>Note 2</em>: We use <code class="docutils literal notranslate"><span class="pre">np.random.seed</span></code> function for reproducibility. The same random number will be used the next time we run the cell!</p>
<p>Let’s check how the data looks like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="p">[:</span><span class="mi">10</span><span class="p">,:]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 2.47143516,  1.52704645],
       [ 0.80902431,  1.7111124 ],
       [ 3.43270697,  0.78245452],
       [ 1.6873481 ,  3.63779121],
       [ 1.27941127, -0.74213763],
       [ 2.88716294,  0.90556519],
       [ 2.85958841,  2.43118375],
       [ 1.3634765 ,  1.59275845],
       [ 2.01569637,  1.1702969 ],
       [-0.24268495, -0.75170595]])
</pre></div>
</div>
</div>
</div>
<p>Nice, we have our two columns vectors.</p>
<p>Now, we can check that the distributions are normal:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">A</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#53BB04&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">A</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;#0A98BE&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_8266/3009214735.py:1: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(A[:,0], color=&quot;#53BB04&quot;)
/tmp/ipykernel_8266/3009214735.py:2: UserWarning: 

`distplot` is a deprecated function and will be removed in seaborn v0.14.0.

Please adapt your code to use either `displot` (a figure-level function with
similar flexibility) or `histplot` (an axes-level function for histograms).

For a guide to updating your code to use the new functions, please see
https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751

  sns.distplot(A[:,1], color=&quot;#0A98BE&quot;)
</pre></div>
</div>
<img alt="../../../../_images/83ce772262e2cc9543838666d132888347d6bb8aa85cd2e55f71631375f426d3.png" src="../../../../_images/83ce772262e2cc9543838666d132888347d6bb8aa85cd2e55f71631375f426d3.png" />
</div>
</div>
<p>Looks good! We can see that the distributions have equivalent standard deviations but different means (1 and 2). So that’s exactly what we have asked for!</p>
<p>Now we can plot our dataset and its covariance matrix with our function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plotDataAndCov</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Covariance matrix:
 [[ 0.95171641 -0.0447816 ]
 [-0.0447816   0.87959853]]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_8266/2462129946.py:8: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.
  ax0 = plt.subplot(2, 2, 1)
/tmp/ipykernel_8266/2462129946.py:14: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.
  ax1 = plt.subplot(2, 2, 2)
</pre></div>
</div>
<img alt="../../../../_images/ed6789ee33b13c48afb2b8ef1e44adada5e2581be0a27fbc860f89a166db295e.png" src="../../../../_images/ed6789ee33b13c48afb2b8ef1e44adada5e2581be0a27fbc860f89a166db295e.png" />
</div>
</div>
<p>We can see on the scatterplot that the two dimensions are uncorrelated. Note that we have one dimension with a mean of 1 and the other with the mean of 2. Also, the covariance matrix shows that the variance of each variable is very large (around 1) and the covariance of columns 1 and 2 is very small (around 0). Since we insured that the two vectors are independent this is coherent (the opposite is not necessarily true: a covariance of 0 doesn’t guaranty independency (see <a class="reference external" href="https://stats.stackexchange.com/questions/12842/covariance-and-independence">here</a>).</p>
</section>
<section id="correlated-data">
<h3>Correlated data<a class="headerlink" href="#correlated-data" title="Permalink to this heading">#</a></h3>
<p>Now, let’s construct dependent data by specifying one column from the other one.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">b1</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">b1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span><span class="o">/</span><span class="mf">2.</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">b1</span><span class="p">,</span> <span class="n">b2</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
<span class="n">plotDataAndCov</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Covariance matrix:
 [[0.95171641 0.92932561]
 [0.92932561 1.12683445]]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_8266/2462129946.py:8: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.
  ax0 = plt.subplot(2, 2, 1)
/tmp/ipykernel_8266/2462129946.py:14: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.
  ax1 = plt.subplot(2, 2, 2)
</pre></div>
</div>
<img alt="../../../../_images/0cbcb51d3f6a20e17f364443e3b1a76d170194e04fbb19bae45ad4400315e60f.png" src="../../../../_images/0cbcb51d3f6a20e17f364443e3b1a76d170194e04fbb19bae45ad4400315e60f.png" />
</div>
</div>
<p>The correlation between the two dimensions is visible on the scatter plot. We can see that a line could be drawn and used to predict <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> from <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> and vice versa. The covariance matrix is not diagonal (there are non-zero cells outside of the diagonal). That means that the covariance between dimensions is non-zero.</p>
<p>That’s great! ⚡️ We now have all the tools to see different preprocessing techniques.</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="preprocessing">
<h1>2. Preprocessing<a class="headerlink" href="#preprocessing" title="Permalink to this heading">#</a></h1>
<section id="a-mean-normalization">
<h2>A. Mean normalization<a class="headerlink" href="#a-mean-normalization" title="Permalink to this heading">#</a></h2>
<p>Mean normalization is just removing the mean from each observation.</p>
<div class="math notranslate nohighlight">
\[
\bs{X'} = \bs{X} - \bar{x}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bs{X'}\)</span> is the normalized dataset, <span class="math notranslate nohighlight">\(\bs{X}\)</span> the original dataset and <span class="math notranslate nohighlight">\(\bar{x}\)</span> the mean of <span class="math notranslate nohighlight">\(\bs{X}\)</span>.</p>
<p>It will have the effect of centering the data around 0. We will create the function <code class="docutils literal notranslate"><span class="pre">center()</span></code> to do that:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">center</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">newX</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">newX</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s give it a try with the matrix <span class="math notranslate nohighlight">\(\bs{B}\)</span> we have created above:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">BCentered</span> <span class="o">=</span> <span class="n">center</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Before:</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">plotDataAndCov</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;After:</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">plotDataAndCov</span><span class="p">(</span><span class="n">BCentered</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Before:


Covariance matrix:
 [[0.95171641 0.92932561]
 [0.92932561 1.12683445]]
After:


Covariance matrix:
 [[0.95171641 0.92932561]
 [0.92932561 1.12683445]]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_8266/2462129946.py:8: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.
  ax0 = plt.subplot(2, 2, 1)
/tmp/ipykernel_8266/2462129946.py:14: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.
  ax1 = plt.subplot(2, 2, 2)
</pre></div>
</div>
<img alt="../../../../_images/0cbcb51d3f6a20e17f364443e3b1a76d170194e04fbb19bae45ad4400315e60f.png" src="../../../../_images/0cbcb51d3f6a20e17f364443e3b1a76d170194e04fbb19bae45ad4400315e60f.png" />
<img alt="../../../../_images/488ba53d92fad7c9e2fbebc566fa6a842a45a9f8dd9f0943013367354449fa79.png" src="../../../../_images/488ba53d92fad7c9e2fbebc566fa6a842a45a9f8dd9f0943013367354449fa79.png" />
</div>
</div>
<p>The first plot shows again the original data <span class="math notranslate nohighlight">\(\bs{B}\)</span> and the second plot shows the centered data (look at the scale).</p>
</section>
<section id="b-standardization">
<h2>B. Standardization<a class="headerlink" href="#b-standardization" title="Permalink to this heading">#</a></h2>
<p>The standardization is used to put all features on the same scale. The way to do it is to divide each zero-centered dimension by its standard deviation.</p>
<div class="math notranslate nohighlight">
\[
\bs{X'} = \frac{\bs{X} - \bar{x}}{\sigma_{\bs{X}}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bs{X'}\)</span> is the standardized dataset, <span class="math notranslate nohighlight">\(\bs{X}\)</span> the original dataset, <span class="math notranslate nohighlight">\(\bar{x}\)</span> the mean of <span class="math notranslate nohighlight">\(\bs{X}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{\bs{X}}\)</span> the standard deviation of <span class="math notranslate nohighlight">\(\bs{X}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">standardize</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">newX</span> <span class="o">=</span> <span class="n">center</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">newX</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s create another dataset with a different scale to check that it is working.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">c1</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
<span class="n">c2</span> <span class="o">=</span> <span class="n">c1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span><span class="o">/</span><span class="mf">2.</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>

<span class="n">plotDataAndCov</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Covariance matrix:
 [[0.95171641 0.83976242]
 [0.83976242 6.22529922]]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_8266/2462129946.py:8: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.
  ax0 = plt.subplot(2, 2, 1)
/tmp/ipykernel_8266/2462129946.py:14: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.
  ax1 = plt.subplot(2, 2, 2)
</pre></div>
</div>
<img alt="../../../../_images/4f21d81137d922b4ed5f5fd1677f3ebb529f5baafa662879451c834116e033db.png" src="../../../../_images/4f21d81137d922b4ed5f5fd1677f3ebb529f5baafa662879451c834116e033db.png" />
</div>
</div>
<p>We can see that the scales of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are different. Note also that the correlation seems smaller because of the scale differences. Now let’s standardise it:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">CStandardized</span> <span class="o">=</span> <span class="n">standardize</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>

<span class="n">plotDataAndCov</span><span class="p">(</span><span class="n">CStandardized</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Covariance matrix:
 [[1.         0.34500274]
 [0.34500274 1.        ]]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_8266/2462129946.py:8: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.
  ax0 = plt.subplot(2, 2, 1)
/tmp/ipykernel_8266/2462129946.py:14: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.
  ax1 = plt.subplot(2, 2, 2)
</pre></div>
</div>
<img alt="../../../../_images/ae03ef4a50e1743b276c68932c07faa8e4ba18347546dcc1dabd1ed0c81989f0.png" src="../../../../_images/ae03ef4a50e1743b276c68932c07faa8e4ba18347546dcc1dabd1ed0c81989f0.png" />
</div>
</div>
<p>Looks good! You can see that the scales are the same and that the dataset is zero-centered according to both axes. Now, have a look at the covariance matrix: you can see that the variance of each coordinate (the top-left cell and the bottom-right cell) is equal to 1. By the way, this new covariance matrix is actually the correlation matrix!💥 The Pearson correlation coefficient between the two variables (<span class="math notranslate nohighlight">\(\bs{c1}\)</span> and <span class="math notranslate nohighlight">\(\bs{c2}\)</span>) is 0.54220151.</p>
</section>
<section id="c-whitening">
<h2>C. Whitening<a class="headerlink" href="#c-whitening" title="Permalink to this heading">#</a></h2>
<p>Whitening or sphering data means that we want to transform it in a way to have a covariance matrix that is the identity matrix (1 in the diagonal and 0 for the other cells; <a class="reference external" href="https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.3-Identity-and-Inverse-Matrices/">more details on the identity matrix</a>). It is called whitening in reference to white noise.</p>
<p>Whitening is a bit more complicated but we now have all the tools that we need to do it. It involves the following steps:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>1- Zero-center the data
2- Decorrelate the data
3- Rescale the data
</pre></div>
</div>
<p>Let’s take again <span class="math notranslate nohighlight">\(\bs{C}\)</span> and try to do these steps.</p>
<section id="zero-centering">
<h3>1. Zero-centering<a class="headerlink" href="#zero-centering" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">CCentered</span> <span class="o">=</span> <span class="n">center</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>

<span class="n">plotDataAndCov</span><span class="p">(</span><span class="n">CCentered</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Covariance matrix:
 [[0.95171641 0.83976242]
 [0.83976242 6.22529922]]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_8266/2462129946.py:8: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.
  ax0 = plt.subplot(2, 2, 1)
/tmp/ipykernel_8266/2462129946.py:14: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.
  ax1 = plt.subplot(2, 2, 2)
</pre></div>
</div>
<img alt="../../../../_images/9e5b3b6248edb389847d1db5faa2aedd390d2597dbf824a74632c999c9a5e149.png" src="../../../../_images/9e5b3b6248edb389847d1db5faa2aedd390d2597dbf824a74632c999c9a5e149.png" />
</div>
</div>
</section>
<section id="decorrelate">
<h3>2. Decorrelate<a class="headerlink" href="#decorrelate" title="Permalink to this heading">#</a></h3>
<p>At this point, we need to decorrelate our data. Intuitively, it means that we want to rotate the data until there is no correlation anymore. Look at the following cartoon to see what I mean:</p>
<p><img alt="" src="../../../../_images/rotation.png" />
<em>The left plot shows correlated data. For instance, if you take a data point with a big <span class="math notranslate nohighlight">\(x\)</span> value, chances are that <span class="math notranslate nohighlight">\(y\)</span> will also be quite big. Now take all data points and do a rotation (maybe around 45 degrees counterclockwise): the new data (plotted on the right) is not correlated anymore.</em></p>
<p>The question is: how could we find the right rotation in order to get the uncorrelated data? Actually, it is exactly what the <strong>eigenvectors</strong> of the covariance matrix do: they indicate the direction where the spread of the data is at its maximum:</p>
<p><img alt="" src="../../../../_images/maxVar.png" /></p>
<p><em>The eigenvectors of the covariance matrix give you the direction that maximizes the variance. The direction of the green line is where the variance is maximum. Just look at the smallest and largest point projected on this line: the spread is big. Compare that with the projection on the orange line: the spread is very small.</em></p>
<p>For more details about the eigendecomposition, see <a class="reference external" href="https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.7-Eigendecomposition/">this post</a>.</p>
<p>So we can decorrelate the data by projecting it on the eigenvectors basis. This will have the effect to apply the rotation needed and remove correlations between the dimensions. Here are the steps:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>1- Calculate the covariance matrix
2- Calculate the eigenvectors of the covariance matrix
3- Apply the matrix of eigenvectors to the data (this will apply the rotation)
</pre></div>
</div>
<p>Let’s pack that into a function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">decorrelate</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">newX</span> <span class="o">=</span> <span class="n">center</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="c1"># Calculate the eigenvalues and eigenvectors of the covariance matrix</span>
    <span class="n">eigVals</span><span class="p">,</span> <span class="n">eigVecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>
    <span class="c1"># Apply the eigenvectors to X</span>
    <span class="n">decorrelated</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">eigVecs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">decorrelated</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s try to decorrelate our zero-centered matrix <span class="math notranslate nohighlight">\(\bs{C}\)</span> to see it in action:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plotDataAndCov</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">CDecorrelated</span> <span class="o">=</span> <span class="n">decorrelate</span><span class="p">(</span><span class="n">CCentered</span><span class="p">)</span>
<span class="n">plotDataAndCov</span><span class="p">(</span><span class="n">CDecorrelated</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Covariance matrix:
 [[0.95171641 0.83976242]
 [0.83976242 6.22529922]]
Covariance matrix:
 [[8.21222171e-01 8.88178420e-17]
 [8.88178420e-17 6.35579346e+00]]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_8266/2462129946.py:8: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.
  ax0 = plt.subplot(2, 2, 1)
/tmp/ipykernel_8266/2462129946.py:14: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.
  ax1 = plt.subplot(2, 2, 2)
</pre></div>
</div>
<img alt="../../../../_images/feb1bb0f01f706d3f73ed5e588cb8c065f61ec9274959277600040e878bf688f.png" src="../../../../_images/feb1bb0f01f706d3f73ed5e588cb8c065f61ec9274959277600040e878bf688f.png" />
<img alt="../../../../_images/5931ff19c8ddeee589f86867f00dbe7e7d8d442bd8edc1b87509dfaeccfff2ed.png" src="../../../../_images/5931ff19c8ddeee589f86867f00dbe7e7d8d442bd8edc1b87509dfaeccfff2ed.png" />
</div>
</div>
<p>Nice! This is working 🎄</p>
<p>We can see that the correlation is not here anymore and that the covariance matrix (now a diagonal matrix) confirms that the covariance between the two dimensions is equal to 0.</p>
</section>
<section id="rescale-the-data">
<h3>3. Rescale the data<a class="headerlink" href="#rescale-the-data" title="Permalink to this heading">#</a></h3>
<p>The next step is to scale the uncorrelated matrix in order to obtain a covariance matrix corresponding to the identity matrix (ones on the diagonal and zeros on the other cells). To do that we scale our decorrelated data by dividing each dimension by the square-root of its corresponding eigenvalue.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">whiten</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">newX</span> <span class="o">=</span> <span class="n">center</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="c1"># Calculate the eigenvalues and eigenvectors of the covariance matrix</span>
    <span class="n">eigVals</span><span class="p">,</span> <span class="n">eigVecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>
    <span class="c1"># Apply the eigenvectors to X</span>
    <span class="n">decorrelated</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">eigVecs</span><span class="p">)</span>
    <span class="c1"># Rescale the decorrelated data</span>
    <span class="n">whitened</span> <span class="o">=</span> <span class="n">decorrelated</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">eigVals</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">whitened</span>
</pre></div>
</div>
</div>
</div>
<p><em>Note:</em> we add a small value (here <span class="math notranslate nohighlight">\(10^{-5}\)</span>) to avoid the division by <span class="math notranslate nohighlight">\(0\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">CWhitened</span> <span class="o">=</span> <span class="n">whiten</span><span class="p">(</span><span class="n">CCentered</span><span class="p">)</span>

<span class="n">plotDataAndCov</span><span class="p">(</span><span class="n">CWhitened</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Covariance matrix:
 [[9.99987823e-01 1.62832710e-17]
 [1.62832710e-17 9.99998427e-01]]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_8266/2462129946.py:8: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.
  ax0 = plt.subplot(2, 2, 1)
/tmp/ipykernel_8266/2462129946.py:14: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.
  ax1 = plt.subplot(2, 2, 2)
</pre></div>
</div>
<img alt="../../../../_images/8fde4cc9372821ef5ed27024200bbe464aa4c354adc17a17d3f50bc3f537f49e.png" src="../../../../_images/8fde4cc9372821ef5ed27024200bbe464aa4c354adc17a17d3f50bc3f537f49e.png" />
</div>
</div>
<p>Hooray! We can see that with the covariance matrix that this is all good. We have something that really looks to the identity matrix (<span class="math notranslate nohighlight">\(1\)</span> on the diagonal and <span class="math notranslate nohighlight">\(0\)</span> elsewhere). 🌵</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="image-whitening">
<h1>3. Image whitening<a class="headerlink" href="#image-whitening" title="Permalink to this heading">#</a></h1>
<p>We will see how whitening can be applied to preprocess image dataset. To do so we will use the paper of <a class="reference external" href="https://ieeexplore.ieee.org/document/7808140/">Pal &amp; Sudeep (2016)</a> where they give some details about the process. This preprocessing technique is called Zero component analysis (ZCA).</p>
<p>Check out the paper, but here is the kind of result they got:</p>
<p>![](images/whitening-images-cifar10-pal-sudeep.png” width=“800” alt=“Whitening images from the CIFAR10 dataset. Results from the paper of Pal &amp; Sudeep (2016).”&gt;
<em>Whitening images from the CIFAR10 dataset. Results from the paper of Pal &amp; Sudeep (2016). The original images (left) and the images after the ZCA (right) are shown.</em></p>
<p>First thing first: we will load images from the CIFAR dataset. This dataset is available from Keras but you can also download it <a class="reference external" href="https://www.cs.toronto.edu/~kriz/cifar.html">here</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.datasets</span> <span class="kn">import</span> <span class="n">cifar10</span>

<span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">cifar10</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>

<span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz
170498071/170498071 [==============================] - 4s 0us/step
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2023-10-09 14:41:28.954926: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-09 14:41:29.119150: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(50000, 32, 32, 3)
</pre></div>
</div>
</div>
</div>
<p>The training set of the CIFAR10 dataset contains 50000 images. The shape of <code class="docutils literal notranslate"><span class="pre">X_train</span></code> is (50000, 32, 32, 3). Each image is 32px by 32px and each pixel contains 3 dimensions (R, G, B). Each value is the brightness of the corresponding color between 0 and 255.</p>
<p>We will start by selecting only a subset of the images, let’s say 1000:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:</span><span class="mi">1000</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1000, 32, 32, 3)
</pre></div>
</div>
</div>
</div>
<p>That’s better! Now we will reshape the array to have flat image data with one image per row. Each image will be (1, 3072) because <span class="math notranslate nohighlight">\(32 \times 32 \times 3 = 3072\)</span>. Thus, the array containing all images will be (1000, 3072):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1000, 3072)
</pre></div>
</div>
</div>
</div>
<p>The next step is to be able to see the images. The function <code class="docutils literal notranslate"><span class="pre">imshow()</span></code> from Matplotlib (<a class="reference external" href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html">doc</a>) can be used to show images. It needs images with the shape (<span class="math notranslate nohighlight">\(M \times N \times 3\)</span>) so let’s create a function to reshape the images and be able to visualize them from the shape (1, 3072).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plotImage</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>For instance, let’s plot one of the images we have loaded:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plotImage</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="p">:])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../_images/778b47ccd21c934280cc1bd4f1d3da4a0b8ac1c7adac3ac864ae57b5ef505312.png" src="../../../../_images/778b47ccd21c934280cc1bd4f1d3da4a0b8ac1c7adac3ac864ae57b5ef505312.png" />
</div>
</div>
<p>Cute! 🌵</p>
<p>We can now implement the whitening of the images. <a class="reference external" href="https://ieeexplore.ieee.org/document/7808140/">Pal &amp; Sudeep (2016)</a> describe the process:</p>
<ol class="arabic simple">
<li><p>The first step is to rescale the images to obtain the range [0, 1] by dividing by 255 (the maximum value of the pixels).</p></li>
</ol>
<p>Remind that the formula to obtain the range [0, 1] is:</p>
<div class="math notranslate nohighlight">
\[\frac{data - min(data)}{max(data) - min(data)}\]</div>
<p>but here, the minimum value is 0, so this leads to:</p>
<div class="math notranslate nohighlight">
\[\frac{data}{max(data)} = \frac{data}{255}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_norm</span> <span class="o">=</span> <span class="n">X</span> <span class="o">/</span> <span class="mf">255.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;X.min()&#39;</span><span class="p">,</span> <span class="n">X_norm</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;X.max()&#39;</span><span class="p">,</span> <span class="n">X_norm</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>X.min() 0.0
X.max() 1.0
</pre></div>
</div>
</div>
</div>
<section id="mean-subtraction-per-pixel-or-per-image">
<h2>Mean subtraction: per-pixel or per-image?<a class="headerlink" href="#mean-subtraction-per-pixel-or-per-image" title="Permalink to this heading">#</a></h2>
<p>Ok cool, the range of our pixel values is between 0 and 1 now. The next step is:</p>
<ol class="arabic simple" start="2">
<li><p>Subtract the mean from all image.</p></li>
</ol>
<p>Be careful here 🌪🎢:</p>
<p>One way to do it is to take each image and remove the mean of this image from every pixel (<a class="reference external" href="https://www.computer.org/csdl/proceedings/iccv/2009/4420/00/05459469.pdf">Jarrett et al., 2009</a>). The intuition behind this process is that it centers the pixels of each image around 0.</p>
<p>Another way to do it is to take each of the 3072 pixels that we have (32 by 32 pixels for R, G and B) for every image and subtract the mean of that pixel across all images. This is called per-pixel mean subtraction. This time, each pixel will be centered around 0 <em>according to all images</em>. When you will feed your network with the images, each pixel is considered as a different feature. With the per-pixel mean subtraction, we have centered each feature (pixel) around 0. This technique is commonly used (e.g <a class="reference external" href="http://proceedings.mlr.press/v28/wan13.html">Wan et al., 2013</a>).</p>
<p>We will now do the per-pixel mean subtraction from our 1000 images. Our data are organized with these dimensions (images, pixels). It was (1000, 3072) because there are 1000 images with <span class="math notranslate nohighlight">\(32 \times 32 \times 3 = 3072\)</span> pixels. The mean per-pixel can thus be obtained from the first axis:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_norm</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(3072,)
</pre></div>
</div>
</div>
</div>
<p>This gives us 3072 values which is the number of means: one per pixel. Let’s see the kind of values we have:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_norm</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.5234    , 0.54323137, 0.5274    , ..., 0.50369804, 0.50011765,
       0.45227451])
</pre></div>
</div>
</div>
</div>
<p>This is near 0.5 because we already have normalized to the range [0, 1]. However, we still need to remove the mean from each pixel:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_norm</span> <span class="o">=</span> <span class="n">X_norm</span> <span class="o">-</span> <span class="n">X_norm</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Just to convince ourselves that it worked, we will compute the mean of the first pixel. Let’s hope that it is 0.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_norm</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-5.30575583e-16, -5.98021632e-16, -4.23439062e-16, ...,
       -1.81965554e-16, -2.49800181e-16,  3.98570066e-17])
</pre></div>
</div>
</div>
</div>
<p>This is not exactly 0 but it is small enough that we can consider that it worked! 🌵</p>
<p>Now we want to calculate the covariance matrix of the zero-centered data. Like we have seen above, we can calculate it with the <code class="docutils literal notranslate"><span class="pre">np.cov()</span></code> function from Numpy.</p>
<p>There are two possible correlation matrices that we can calculate from the matrix <span class="math notranslate nohighlight">\(\bs{X}\)</span>: either the correlation between rows or between columns. In our case, each row of the matrix <span class="math notranslate nohighlight">\(\bs{X}\)</span> is an image, so the rows of the matrix correspond to the observations and the columns of the matrix corresponds to the features (the images pixels). We want to calculate the correlation between the pixels because the goal of the whitening is to remove these correlations to force the algorithm to focus on higher-order relations.</p>
<p>To do so, we will tell this to Numpy with the parameter <code class="docutils literal notranslate"><span class="pre">rowvar=False</span></code> (see the <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.cov.html">doc</a>): it will use the columns as variables (or features) and the rows as observations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_norm</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The covariance matrix should have a shape of 3072 by 3072 to represent the correlation between each pair of pixels (and there are 3072 pixels):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cov</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(3072, 3072)
</pre></div>
</div>
</div>
</div>
<p>Now the magic part: we will calculate the singular values and vectors of the covariance matrix and use them to rotate our dataset. Have a look at <a class="reference external" href="https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.8-Singular-Value-Decomposition/">my post</a> on the singular value decomposition if you need more details!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">U</span><span class="p">,</span><span class="n">S</span><span class="p">,</span><span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>In the paper, they used the following equation:</p>
<div class="math notranslate nohighlight">
\[
\bs{X}_{ZCA} = \bs{U}.diag(\frac{1}{\sqrt{diag(\bs{S}) + \epsilon}}).\bs{U^\text{T}.X}
\]</div>
<p>with <span class="math notranslate nohighlight">\(\bs{U}\)</span> the left singular vectors, and <span class="math notranslate nohighlight">\(\bs{S}\)</span> the singular values of the covariance of the initial normalized dataset of images and <span class="math notranslate nohighlight">\(\bs{X}\)</span> the normalized dataset. <span class="math notranslate nohighlight">\(\epsilon\)</span> (<em>epsilon</em>) is an hyper-parameter called the whitening coefficient. <span class="math notranslate nohighlight">\(diag(a)\)</span> corresponds to a matrix with the vector <span class="math notranslate nohighlight">\(a\)</span> as a diagonal and 0 in all other cells.</p>
<p>We will try to implement this equation. Let’s start by checking the dimensions of the SVD:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">U</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">S</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(3072, 3072) (3072,)
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\bs{S}\)</span> is a vector containing 3072 elements (the singular values). <span class="math notranslate nohighlight">\(diag(\bs{S})\)</span> will thus be of shape (3072, 3072) with <span class="math notranslate nohighlight">\(\bs{S}\)</span> as the diagonal:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">S</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">shape:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">S</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[5.46967832e+01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 2.02861434e+01 0.00000000e+00 ... 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 1.24476904e+01 ... 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 ...
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 5.20778268e-15
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00
  4.72611355e-15 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00
  0.00000000e+00 1.33962323e-15]]

shape: (3072, 3072)
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(diag(\frac{1}{\sqrt{diag(\bs{S}) + \epsilon}})\)</span> is also of shape (3072, 3072) as well as <span class="math notranslate nohighlight">\(\bs{U}\)</span> and <span class="math notranslate nohighlight">\(\bs{U^{\text{T}}}\)</span>. We have seen also that <span class="math notranslate nohighlight">\(\bs{X}\)</span> has the shape (1000, 3072) and we need to transpose it to have (3072, 1000). The shape of <span class="math notranslate nohighlight">\(\bs{X}_{ZCA}\)</span> is thus:</p>
<div class="math notranslate nohighlight">
\[
(3072, 3072) . (3072, 3072) . (1000, 3072)^{\text{T}} = (3072, 3072) . (3072, 1000) = (3072, 1000)
\]</div>
<p>which corresponds to the shape of the initial dataset after transposition. Nice!</p>
<p>We have:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">X_ZCA</span> <span class="o">=</span> <span class="n">U</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="mf">1.0</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">S</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)))</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">U</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_norm</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s rescale the images:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_ZCA_rescaled</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_ZCA</span> <span class="o">-</span> <span class="n">X_ZCA</span><span class="o">.</span><span class="n">min</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">X_ZCA</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">X_ZCA</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;min:&#39;</span><span class="p">,</span> <span class="n">X_ZCA_rescaled</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;max:&#39;</span><span class="p">,</span> <span class="n">X_ZCA_rescaled</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>min: 0.0
max: 1.0
</pre></div>
</div>
</div>
</div>
<p>Finally, we can look at the effect of whitening by comparing an image before and after whitening:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plotImage</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="p">:])</span>
<span class="n">plotImage</span><span class="p">(</span><span class="n">X_ZCA_rescaled</span><span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="p">:])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../_images/778b47ccd21c934280cc1bd4f1d3da4a0b8ac1c7adac3ac864ae57b5ef505312.png" src="../../../../_images/778b47ccd21c934280cc1bd4f1d3da4a0b8ac1c7adac3ac864ae57b5ef505312.png" />
<img alt="../../../../_images/f9eea7d94b710437f4c5703da65af98d76f3098964a6fcb76ad1f69476bc4234.png" src="../../../../_images/f9eea7d94b710437f4c5703da65af98d76f3098964a6fcb76ad1f69476bc4234.png" />
</div>
</div>
<p>Hooray! That’s great!⚡️It looks like the images from the paper <a class="reference external" href="https://ieeexplore.ieee.org/document/7808140/">Pal &amp; Sudeep (2016)</a>. They used <code class="docutils literal notranslate"><span class="pre">epsilon</span> <span class="pre">=</span> <span class="pre">0.1</span></code>. You can try other values to see the effect on the image.</p>
<p>That’s all! 🌴</p>
<p>I hope that you found something interesting in this notebook! You can find it in a nicer layout on my <a class="reference external" href="https://hadrienj.github.io/posts/Preprocessing-data-for-machine-learning-and-deep-learning/">blog</a>, along with other articles!</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="references">
<h1>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h1>
<p><a class="reference external" href="https://www.computer.org/csdl/proceedings/iccv/2009/4420/00/05459469.pdf">K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun, “What is the best multi-stage architecture for object recognition?,” in 2009 IEEE 12th International Conference on Computer Vision, 2009, pp. 2146–2153.
</a></p>
<p><a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.222.9220&amp;rep=rep1&amp;type=pdf">A. Krizhevsky, “Learning Multiple Layers of Features from Tiny Images,” Master’s thesis, University of Tront, 2009.
</a></p>
<p><a class="reference external" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">Y. A. LeCun, L. Bottou, G. B. Orr, and K.-R. Müller, “Efficient BackProp,” in Neural Networks: Tricks of the Trade, Springer, Berlin, Heidelberg, 2012, pp. 9–48.
</a></p>
<p><a class="reference external" href="https://ieeexplore.ieee.org/document/7808140/">K. K. Pal and K. S. Sudeep, “Preprocessing for image classification by convolutional neural networks,” in 2016 IEEE International Conference on Recent Trends in Electronics, Information Communication Technology (RTEICT), 2016, pp. 1778–1781.
</a></p>
<p><a class="reference external" href="http://proceedings.mlr.press/v28/wan13.html">L. Wan, M. Zeiler, S. Zhang, Y. L. Cun, and R. Fergus, “Regularization of Neural Networks using DropConnect,” in International Conference on Machine Learning, 2013, pp. 1058–1066.
</a></p>
<p>And also these great resources and QA:</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Whitening_transformation">Wikipedia - Whitening transformation</a></p>
<p><a class="reference external" href="http://cs231n.github.io/neural-networks-2/">CS231 - Convolutional Neural Networks for Visual Recognition</a></p>
<p><a class="reference external" href="https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/">Dustin Stansbury - The Clever Machine</a></p>
<p><a class="reference external" href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/">Some details about the covariance matrix</a></p>
<p><a class="reference external" href="https://stackoverflow.com/questions/41635737/is-this-the-correct-way-of-whitening-an-image-in-python">SO - Image whitening in Python</a></p>
<p><a class="reference external" href="http://ufldl.stanford.edu/wiki/index.php/Data_Preprocessing">Mean normalization per image or from the entire dataset</a></p>
<p><a class="reference external" href="https://stackoverflow.com/questions/29743523/subtract-mean-from-image">Mean subtraction - all images or per image?</a></p>
<p><a class="reference external" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">Why centering is important - See section 4.3</a></p>
<p><a class="reference external" href="https://www.kaggle.com/nicw102168/exploring-zca-and-color-image-whitening/notebook">Kaggle kernel on ZCA</a></p>
<p><a class="reference external" href="https://github.com/keras-team/keras-preprocessing/blob/b9d142456a64ef228475f07cb2f2d38fd05bd249/keras_preprocessing/image.py#L1254:L1257">How ZCA is implemented in Keras</a></p>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "pantelis/data-mining",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./aiml-common/lectures/pca/whitening"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Data Preprocessing</p>
      </div>
    </a>
    <a class="right-next"
       href="../../recommenders/recommenders-intro/_index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Introduction to Recommender Systems</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Preprocessing: from covariance matrix to image whitening</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#background">1. Background</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-variance-and-covariance">A. Variance and covariance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1">Example 1.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-covariance-matrix-with-the-dot-product">Finding the covariance matrix with the dot product</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#b-visualize-data-and-covariance-matrices">B. Visualize data and covariance matrices</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#c-simulating-data">C. Simulating data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#uncorrelated-data">Uncorrelated data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#correlated-data">Correlated data</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocessing">2. Preprocessing</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-mean-normalization">A. Mean normalization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#b-standardization">B. Standardization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#c-whitening">C. Whitening</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-centering">1. Zero-centering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decorrelate">2. Decorrelate</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rescale-the-data">3. Rescale the data</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#image-whitening">3. Image whitening</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-subtraction-per-pixel-or-per-image">Mean subtraction: per-pixel or per-image?</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Pantelis Monogioudis, Ph.D
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794"></script>
<script src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>